import matplotlib.pyplot as plt
from qiskit_ibm_runtime import QiskitRuntimeService
import numpy as np
import json
from datetime import datetime

# ==========================================
# 1. é…ç½®åŒºåŸŸ (å¡«å…¥ä½ çš„ Job IDs)
# ==========================================
JOB_IDS = [
    "d5f2mi4pe0pc73ajhqug", 
    "d5f2min67pic7382l3n0" 
]

# ç†è®ºéšæœºåº•å™ª (3æ¯”ç‰¹ç³»ç»Ÿï¼Œéšæœºæ¦‚ç‡ = 1/8 = 0.125)
RANDOM_BASELINE = 1 / 8 

def fetch_and_visualize():
    # è‡ªåŠ¨åŠ è½½æœ¬åœ°è´¦æˆ·
    try:
        service = QiskitRuntimeService()
    except:
        service = QiskitRuntimeService(channel="ibm_quantum_platform")
        
    print(f"ğŸ”— å·²è¿æ¥ IBM Quantum, æ­£åœ¨ä»è§†ç•Œè¾¹ç¼˜æå–æ•°æ®...")
    
    combined_data = {"000": 0}
    total_shots_all = 0
    job_results = []

    # --- æ­¥éª¤ A: æŠ“å–å¹¶åˆå¹¶æ•°æ® ---
    for jid in JOB_IDS:
        try:
            job = service.job(jid)
            
            # === å…³é”®ä¿®å¤ç‚¹ ===
            # è·å–åŸå§‹çŠ¶æ€
            raw_status = job.status()
            # å…¼å®¹å¤„ç†ï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²ç›´æ¥ç”¨ï¼Œå¦‚æœæ˜¯å¯¹è±¡å–.name
            status_str = raw_status if isinstance(raw_status, str) else raw_status.name
            
            print(f"   >> Job {jid}: [{status_str}]")
            
            if status_str == 'DONE':
                # SamplerV2 ç»“æœæå–é€»è¾‘
                result = job.result()
                # æå–ç¬¬ä¸€ä¸ª pub çš„ç»“æœ
                pub_result = result[0] 
                # è·å–æµ‹é‡æ•°æ® (å…¼å®¹ c å’Œ meas å¯„å­˜å™¨å)
                if hasattr(pub_result.data, 'meas'):
                    counts = pub_result.data.meas.get_counts()
                else:
                    # æœ‰æ—¶å€™é»˜è®¤å¯„å­˜å™¨å« c
                    counts = pub_result.data.c.get_counts()
                
                total_shots = sum(counts.values())
                total_shots_all += total_shots
                
                # è®°å½•å…³é”®æŒ‡æ ‡
                p0 = counts.get('000', 0) / total_shots
                job_results.append({
                    "id": jid, 
                    "p0": p0, 
                    "shots": total_shots,
                    "counts": counts
                })
                
                # åˆå¹¶è®¡æ•°
                for k, v in counts.items():
                    combined_data[k] = combined_data.get(k, 0) + v
                    
                print(f"      âœ… æˆåŠŸæ‰“æ! å•æ¬¡ P(0) æ¢å¤ç‡: {p0:.4f} (åŸºå‡†: {RANDOM_BASELINE})")
            elif status_str in ['QUEUED', 'RUNNING', 'VALIDATING']:
                print("      â³ ä»»åŠ¡è¿˜åœ¨æ’é˜Ÿæˆ–è¿è¡Œä¸­ï¼Œè¯·ç¨åå†è¯•ã€‚")
            else:
                print(f"      âš ï¸ ä»»åŠ¡çŠ¶æ€å¼‚å¸¸: {status_str}")
                
        except Exception as e:
            print(f"      âŒ æŠ“å–å¤±è´¥: {e}")

    if total_shots_all == 0:
        print("æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè„šæœ¬ç»“æŸã€‚")
        return

    # --- æ­¥éª¤ B: è®¡ç®—æœ€ç»ˆç»Ÿè®¡é‡ ---
    final_p0 = combined_data.get('000', 0) / total_shots_all
    enhancement = (final_p0 - RANDOM_BASELINE) / RANDOM_BASELINE * 100
    
    print("\n" + "="*40)
    print(f"ğŸŒŒ ã€æœ€ç»ˆå®¡åˆ¤æ—¥æŠ¥å‘Šã€‘ (Total Shots: {total_shots_all})")
    print(f"ğŸŒŒ éšæœºæ··æ²ŒåŸºå‡†: {RANDOM_BASELINE:.4f}")
    print(f"ğŸŒŒ å‡ ä½•é€†è½¬ç»“æœ: {final_p0:.4f}")
    print(f"ğŸ”¥ å› æœä¿¡å·å¢å¼º: +{enhancement:.2f}%")
    print("="*40)

    # --- æ­¥éª¤ C: ä¿å­˜åŸå§‹æ•°æ® (JSON) ---
    export_data = {
        "timestamp": str(datetime.now()),
        "random_baseline": RANDOM_BASELINE,
        "final_stats": {
            "total_shots": total_shots_all,
            "final_p0": final_p0,
            "enhancement_percentage": enhancement
        },
        "merged_counts": combined_data,
        "individual_jobs": job_results
    }
    with open("blackhole_data.json", "w") as f:
        json.dump(export_data, f, indent=4)
    print("ğŸ’¾ åŸå§‹æ•°æ®å·²ä¿å­˜è‡³: blackhole_data.json")

    # --- æ­¥éª¤ D: ç”Ÿæˆ PDF çº§å›¾è¡¨ ---
    generate_plot(combined_data, total_shots_all, final_p0, enhancement)

def generate_plot(counts, total, p0, boost):
    sorted_keys = sorted(counts.keys())
    # ç¡®ä¿ 000 åœ¨æœ€å‰
    if '000' in sorted_keys:
        sorted_keys.remove('000')
        sorted_keys.insert(0, '000')
        
    probs = [counts[k]/total for k in sorted_keys]
    colors = ['#FF4500' if k == '000' else '#1f77b4' for k in sorted_keys]
    
    plt.figure(figsize=(10, 6))
    bars = plt.bar(sorted_keys, probs, color=colors, alpha=0.8, edgecolor='black')
    plt.axhline(y=RANDOM_BASELINE, color='green', linestyle='--', linewidth=2, label='Random Noise Floor')
    
    plt.title(f"Evidence of Causal Reversal via Gamma=0.25 (150 Layers)\nInformation Recovery: {boost:.2f}% above Chaos", fontsize=14)
    plt.xlabel("Quantum States (Bitstrings)", fontsize=12)
    plt.ylabel("Probability Density", fontsize=12)
    plt.legend()
    
    if probs:
        plt.text(0, probs[0] + 0.005, f"{probs[0]:.4f}\n(ANCHOR)", ha='center', fontweight='bold', color='#FF4500')

    plt.text(0.95, 0.95, 'IBM Torino / Heron r1', transform=plt.gca().transAxes, 
             fontsize=10, color='gray', alpha=0.5, ha='right', va='top')

    plt.grid(axis='y', linestyle='--', alpha=0.3)
    plt.tight_layout()
    
    filename = "Causal_Reversal_Verdict.pdf"
    plt.savefig(filename)
    print(f"ğŸ“„ åˆ¤å†³æŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
    plt.show()

if __name__ == "__main__":
    fetch_and_visualize()
